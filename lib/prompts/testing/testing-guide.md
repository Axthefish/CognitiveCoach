# Prompt 测试指南

本指南说明如何测试和优化 CognitiveCoach 的 Prompt 系统。

## 测试环境设置

### 1. 环境变量配置

```bash
# 在 .env.local 中配置
GEMINI_API_KEY=your_api_key_here
NODE_ENV=development
```

### 2. 安装依赖

```bash
npm install
```

### 3. 启动开发服务器

```bash
npm run dev
```

---

## 测试流程

### 阶段一：单元测试

**目标**：验证每个 Prompt 的基础功能

#### Stage 0 测试

1. 打开 `lib/prompts/testing/stage0-test-cases.json`
2. 从测试用例中选择一个场景
3. 在浏览器中手动输入测试数据
4. 观察 AI 的响应
5. 记录结果

**测试checklist**：
- [ ] AI 是否识别出正确的问题域？
- [ ] 生成的问题是否开放式、友好？
- [ ] 对话是否在合理轮次内完成（2-4轮）？
- [ ] 最终的 `PurposeDefinition` 是否清晰准确？

#### Stage 1 测试

1. 使用 Stage 0 的输出作为输入
2. 观察生成的框架
3. 检查权重计算

**测试checklist**：
- [ ] 节点数量是否合理（5-12个）？
- [ ] 权重分布是否合理（核心必修2-4个）？
- [ ] 权重是否基于用户目的（而非通用）？
- [ ] 依赖关系是否正确？
- [ ] 主路径是否清晰？

#### Stage 2 测试

（待 Stage 2 实现后补充）

---

### 阶段二：集成测试

**目标**：测试完整的用户流程

#### 端到端测试场景

**场景1：技能学习**
```
输入："我想学Python"
期望流程：
1. Stage 0: 3轮对话，明确"数据分析目的"
2. Stage 1: 生成框架，NumPy/Pandas高权重
3. Stage 2: 收集基础信息，调整学习路径
```

**场景2：职业转型**
```
输入："我想转行做数据分析"
期望流程：
1. Stage 0: 4-5轮对话，了解背景和约束
2. Stage 1: 生成框架，强调项目经验和面试准备
3. Stage 2: 个性化时间线和里程碑
```

**场景3：问题解决**
```
输入："我是产品经理，想提升数据分析能力"
期望流程：
1. Stage 0: 2-3轮对话，明确产品决策场景
2. Stage 1: 生成框架，侧重业务分析而非技术深度
3. Stage 2: 结合产品工作设计实践项目
```

---

### 阶段三：A/B 测试

**目标**：对比不同 Prompt 变体的效果

#### 测试设计

1. **准备变体**
   - 创建 Prompt 的变体版本
   - 例如：调整温度参数、改变指令措辞、修改示例

2. **收集数据**
   - 使用相同的测试用例
   - 记录每个变体的输出
   - 评估质量指标

3. **对比分析**
   - 哪个变体的问题质量更高？
   - 哪个变体的对话更自然？
   - 哪个变体的输出更准确？

#### 评估维度

| 维度 | 权重 | 评分标准 (1-5) |
|------|------|---------------|
| **准确性** | 30% | 输出是否符合预期 |
| **自然度** | 25% | 对话是否流畅友好 |
| **效率** | 20% | 是否在合理轮次内完成 |
| **完整性** | 15% | 是否收集到所有必要信息 |
| **一致性** | 10% | 重复测试是否稳定 |

---

## 测试记录模板

### 单个测试用例记录

```markdown
## 测试ID: tc001
**日期**: 2025-01-15
**测试人员**: 张三

### 输入
用户输入："我想学Python"

### 实际输出

**Round 1**
- AI问题: "学习Python主要是为了什么？有具体的应用场景吗？"
- 用户回答: "想做数据分析"
- 评分: 4/5（问题开放，但可以更具体）

**Round 2**
- AI问题: "你目前有编程基础吗？每周能投入多少时间学习？"
- 用户回答: "零基础，每周5-10小时"
- 评分: 5/5

**最终输出**
```json
{
  "clarified_purpose": "学习Python用于数据分析",
  "problem_domain": "Python编程学习（数据分析方向）",
  ...
}
```

### 评估
- ✅ 问题域识别准确
- ✅ 对话轮次合理（2轮）
- ⚠️ 可以在第1轮就收集基础和时间信息
- ✅ 最终输出清晰

### 改进建议
1. 可以在初始prompt中就引导AI同时询问多个维度
2. 增加对"数据分析"的进一步澄清（具体应用场景）
```

---

## Prompt 优化流程

### 1. 识别问题

从测试结果中识别问题：
- AI 的问题过于宽泛/过于具体
- 对话轮次过多/过少
- 输出格式不稳定
- 关键信息遗漏

### 2. 分析原因

可能的原因：
- Prompt 指令不够清晰
- 示例不够具体
- 温度参数设置不当
- 缺少约束条件

### 3. 实施改进

改进策略：
- **增加约束**：明确输出格式、字数限制
- **提供示例**：好的示例和差的示例对比
- **调整参数**：temperature, topP, topK
- **重构结构**：分步骤说明任务

### 4. 验证效果

- 使用相同测试用例重新测试
- 对比改进前后的输出质量
- 收集用户反馈

### 5. 文档更新

- 更新 `prompt-templates.md`
- 记录改进原因和效果
- 添加新的测试用例

---

## 常见问题与解决方案

### Q1: AI 不遵循 JSON 格式

**现象**：输出混杂了文本和JSON

**解决方案**：
1. 在 Prompt 中强调"严格按照以下JSON格式"
2. 使用代码块包裹：\`\`\`json ... \`\`\`
3. 在解析时使用正则提取JSON部分
4. 提供明确的格式示例

### Q2: 对话轮次过多

**现象**：需要5+轮对话才能完成目的澄清

**解决方案**：
1. 在每轮追问时收集多个维度信息
2. 提高clarification_threshold
3. 优化"missing_info"判断逻辑
4. 提供更具体的追问策略

### Q3: 权重计算不准确

**现象**：所有节点权重都很高或都很低

**解决方案**：
1. 强调权重要基于用户**具体目的**
2. 提供权重计算的示例
3. 要求AI输出计算过程
4. 添加权重分布验证

### Q4: 输出不稳定

**现象**：相同输入产生差异很大的输出

**解决方案**：
1. 降低 temperature (0.7 → 0.6)
2. 增加约束和示例
3. 使用更具体的指令
4. 考虑使用 seed 参数（如果API支持）

---

## 性能指标

### 目标指标

| 指标 | 目标值 | 当前值 | 状态 |
|------|--------|--------|------|
| Stage 0 完成率 | >90% | TBD | 🟡 |
| 平均对话轮次 | 2-3轮 | TBD | 🟡 |
| 权重准确率 | >85% | TBD | 🟡 |
| 用户满意度 | >4.0/5 | TBD | 🟡 |
| 响应时间 | <3秒 | TBD | 🟡 |

### 监控方式

1. **日志分析**：记录所有AI调用和响应
2. **用户反馈**：收集用户的直接反馈
3. **A/B测试**：对比不同版本的效果
4. **错误追踪**：记录失败案例并分析

---

## 下一步

1. **完善测试用例**：添加更多真实场景
2. **自动化测试**：编写测试脚本
3. **持续监控**：建立监控dashboard
4. **用户研究**：收集真实用户反馈
5. **迭代优化**：基于数据持续改进

---

## 附录：测试脚本示例

```typescript
// tests/prompt-test.ts
import { Stage0Service } from '@/services/stage0-service';

async function testStage0(testCase: TestCase) {
  const service = Stage0Service.getInstance();
  
  console.log(`Testing: ${testCase.id} - ${testCase.scenario}`);
  
  const result = await service.processInitialInput(testCase.user_input);
  
  // 验证输出
  assert(result.data.problemDomain, 'Should have problem domain');
  assert(result.message, 'Should have next question');
  
  console.log('✓ Test passed');
}
```

运行测试：
```bash
npx ts-node tests/prompt-test.ts
```

